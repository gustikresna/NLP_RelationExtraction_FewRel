{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "temefE88BqPS"
      },
      "source": [
        "# **1. Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_YJkPzIQ0Hu-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import json\n",
        "import pprint\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MaxAbsScaler\n",
        "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "import gensim.downloader as api\n",
        "from joblib import load, dump\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taNJ97wVBvAJ"
      },
      "source": [
        "# **2. Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aXzJOgts0fCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdd9eea-e2a8-45c8-e28e-bf3a77157918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FewRel'...\n",
            "remote: Enumerating objects: 565, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 565 (delta 1), reused 0 (delta 0), pack-reused 559\u001b[K\n",
            "Receiving objects: 100% (565/565), 24.68 MiB | 7.01 MiB/s, done.\n",
            "Resolving deltas: 100% (340/340), done.\n",
            "/content/FewRel\n"
          ]
        }
      ],
      "source": [
        "#clone fewrel repository\n",
        "!git clone https://github.com/thunlp/FewRel\n",
        "#change directory\n",
        "%cd FewRel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aiucy-cY2D5b"
      },
      "outputs": [],
      "source": [
        "#load training data\n",
        "with open('./data/train_wiki.json', 'r') as file:\n",
        "    fewrel = json.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBWreagrDsLA"
      },
      "source": [
        "# **3. Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fwG8jxaDzBn"
      },
      "source": [
        "## Set up Data & Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WOCgLgvLDHWu"
      },
      "outputs": [],
      "source": [
        "#set up nlp environment\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kLsmWoL0AL9X"
      },
      "outputs": [],
      "source": [
        "#extract lists of full texts and labels for training data\n",
        "texts, relations = [], []\n",
        "for relation, instances in fewrel.items():\n",
        "    for instance in instances:\n",
        "        texts.append(' '.join(instance['tokens']))  #combine tokens into sentence\n",
        "        relations.append(relation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "68iB77tMF1Kn"
      },
      "outputs": [],
      "source": [
        "#extract lists of heads, tails, and tokens\n",
        "heads, tails, h_seq, t_seq, tokens = [], [], [], [], []\n",
        "for relation, instances in fewrel.items():\n",
        "  for instance in instances:\n",
        "    heads.append(instance['h'][0]) #extract head text\n",
        "    tails.append(instance['t'][0]) #extract tail text\n",
        "    h_seq.append(instance['h'][2][0]) #extract head seq\n",
        "    t_seq.append(instance['t'][2][0]) #extract tail seq\n",
        "    tokens.append(instance['tokens']) #extract token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YTPB94KjJNpD"
      },
      "outputs": [],
      "source": [
        "#combine into dataframe\n",
        "features = {'head': heads, 'tail': tails, 'token': tokens, 'text': texts, 'h_seq':h_seq, 't_seq':t_seq, 'relation': relations}\n",
        "fewrel_df = pd.DataFrame(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYJnAYNmCDXt"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class of NER count transformer for the pipeline\n",
        "class ner_count_transformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        #initialise an empty set to gather all unique NER labels\n",
        "        self.unique_ner_labels = set()\n",
        "\n",
        "        #iterate through the dataframe to collect all unique NER labels\n",
        "        for _, row in X.iterrows():\n",
        "            # Extract text for head and tail\n",
        "            head_text, tail_text = row['head'], row['tail']\n",
        "\n",
        "            #process text for NER and update unique_ner_labels with labels from head and tail\n",
        "            head_doc = nlp(head_text)\n",
        "            tail_doc = nlp(tail_text)\n",
        "            self.unique_ner_labels.update([ent.label_ for ent in head_doc.ents])\n",
        "            self.unique_ner_labels.update([ent.label_ for ent in tail_doc.ents])\n",
        "\n",
        "        #convert set to list to be sorted\n",
        "        self.unique_ner_labels = sorted(list(self.unique_ner_labels))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        #initialise list to store the feature dictionaries for each record\n",
        "        ner_features_list = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            #extract text for head and tail\n",
        "            head_text, tail_text = row['head'], row['tail']\n",
        "\n",
        "            #process text for NER\n",
        "            head_doc = nlp(head_text)\n",
        "            tail_doc = nlp(tail_text)\n",
        "\n",
        "            #initialise counters for both head and tail NER labels\n",
        "            head_ner_count = Counter([ent.label_ for ent in head_doc.ents])\n",
        "            tail_ner_count = Counter([ent.label_ for ent in tail_doc.ents])\n",
        "\n",
        "            #combine counts, adding prefix for head and tail, and ensuring all labels are included\n",
        "            combined_counts = {f'H_{label}': head_ner_count.get(label, 0) for label in self.unique_ner_labels}\n",
        "            combined_counts.update({f'T_{label}': tail_ner_count.get(label, 0) for label in self.unique_ner_labels})\n",
        "\n",
        "            ner_features_list.append(combined_counts)\n",
        "\n",
        "        #convert list of dictionaries to a DataFrame\n",
        "        return pd.DataFrame(ner_features_list)"
      ],
      "metadata": {
        "id": "9sMzwaXdkHbT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALX-L-MlZ0nX"
      },
      "source": [
        "## Part of Speech"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class of POS Tag count transformer for the pipeline\n",
        "class pos_count_transformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        #initialise an empty set to gather all unique POS tags\n",
        "        self.unique_tags = set()\n",
        "\n",
        "        #iterate through the dataframe to collect all unique POS tags\n",
        "        for _, row in X.iterrows():\n",
        "            text, h_seq, t_seq = row['text'], row['h_seq'], row['t_seq']\n",
        "            doc = nlp(text)\n",
        "            #update unique_tags with tags from the entire text\n",
        "            self.unique_tags.update([token.pos_ for token in doc])\n",
        "            #update unique_tags with tags specifically from head and tail sequences\n",
        "            self.unique_tags.update([doc[i].pos_ for i in h_seq if i < len(doc)])\n",
        "            self.unique_tags.update([doc[i].pos_ for i in t_seq if i < len(doc)])\n",
        "\n",
        "        #convert set to list to fix the order\n",
        "        self.unique_tags = sorted(list(self.unique_tags))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        #initialise list to store the feature dictionaries for each record\n",
        "        pos_features_list = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            text, h_seq, t_seq = row['text'], row['h_seq'], row['t_seq']\n",
        "            doc = nlp(text)\n",
        "\n",
        "            #initialize counters for both head and tail sequences\n",
        "            head_pos_count = Counter()\n",
        "            tail_pos_count = Counter()\n",
        "\n",
        "            #count POS tags for head and tail sequences\n",
        "            for index in h_seq:\n",
        "                head_pos_count[doc[index].pos_] += 1\n",
        "\n",
        "            for index in t_seq:\n",
        "                tail_pos_count[doc[index].pos_] += 1\n",
        "\n",
        "            #combine counts, adding prefix for head and tail, and ensuring all tags are included\n",
        "            combined_counts = {f'H_{tag}': head_pos_count.get(tag, 0) for tag in self.unique_tags}\n",
        "            combined_counts.update({f'T_{tag}': tail_pos_count.get(tag, 0) for tag in self.unique_tags})\n",
        "\n",
        "            pos_features_list.append(combined_counts)\n",
        "\n",
        "        #convert list of dictionaries to a DataFrame\n",
        "        return pd.DataFrame(pos_features_list)"
      ],
      "metadata": {
        "id": "s11X6l4yIrxL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3B4A8DHjoUC"
      },
      "source": [
        "## Dependency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class of dependency count transformer for the pipeline\n",
        "class dependency_count_transformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        #initialize an empty set to gather all unique dependency\n",
        "        unique_deps = set()\n",
        "\n",
        "        #iterate through the dataframe to collect all unique dependency\n",
        "        for _, row in X.iterrows():\n",
        "            text = row['text']\n",
        "            doc = nlp(text)\n",
        "            #update unique_deps with all dependency tags from the document\n",
        "            unique_deps.update([token.dep_ for token in doc])\n",
        "\n",
        "        #convert set to list to fix the order\n",
        "        self.unique_deps = sorted(list(unique_deps))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        #initialise list to store the feature dictionaries for each record\n",
        "        dep_counts_list = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            text, h_seq, t_seq = row['text'], row['h_seq'], row['t_seq']\n",
        "            doc = nlp(text)\n",
        "\n",
        "            #initialise counters for both head and tail sequences\n",
        "            head_dep_count = Counter()\n",
        "            tail_dep_count = Counter()\n",
        "\n",
        "            #count dependency for head and tail sequences\n",
        "            for index in h_seq:\n",
        "                head_dep_count[doc[index].dep_] += 1\n",
        "\n",
        "            for index in t_seq:\n",
        "                tail_dep_count[doc[index].dep_] += 1\n",
        "\n",
        "            #combine counts, ensuring all tags are included even if their count is zero\n",
        "            combined_counts = {f'H_{dep}': head_dep_count.get(dep, 0) for dep in self.unique_deps}\n",
        "            combined_counts.update({f'T_{dep}': tail_dep_count.get(dep, 0) for dep in self.unique_deps})\n",
        "\n",
        "            dep_counts_list.append(combined_counts)\n",
        "\n",
        "        #convert list of dictionaries to a DataFrame\n",
        "        return pd.DataFrame(dep_counts_list)"
      ],
      "metadata": {
        "id": "FZ76hLXCJc9t"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebgm3m8nr9-q"
      },
      "source": [
        "## Distance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class of distance calculation transformer for the pipeline\n",
        "class distance_transformer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        #distance does not require learning anything from the training data\n",
        "        return self  #return self to allow pipeline\n",
        "\n",
        "    def transform(self, X):\n",
        "        #initialize an empty list to store the distances\n",
        "        distances = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            h_seq, t_seq = row['h_seq'], row['t_seq']\n",
        "\n",
        "            if h_seq and t_seq:\n",
        "                #grasp start and end sequence\n",
        "                head_end = max(h_seq)\n",
        "                tail_start = min(t_seq)\n",
        "                head_start = min(h_seq)\n",
        "                tail_end = max(t_seq)\n",
        "\n",
        "                #calculate the distance based on their positions\n",
        "                if head_end < tail_start:  #head comes before tail\n",
        "                    distance = tail_start - head_end - 1  #subtract 1 to not count overlapping word\n",
        "                elif tail_end < head_start:  #tail comes before head\n",
        "                    distance = head_start - tail_end - 1  #subtract 1 to not count overlapping word\n",
        "                else:  #overlapping\n",
        "                    distance = 0  #if they overlap, the distance is considered as 0\n",
        "\n",
        "            distances.append(distance)\n",
        "\n",
        "        #convert the list of distances to a DataFrame\n",
        "        return pd.DataFrame(distances, columns=['distance'])"
      ],
      "metadata": {
        "id": "jLpiZ3HLIDb5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ1o8zms9FpR"
      },
      "source": [
        "## Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a class of word embedding transformer for the pipeline\n",
        "class word_embedding_transformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, word_vectors):\n",
        "        self.word_vectors = word_vectors\n",
        "\n",
        "    def get_vector(self, word):\n",
        "        #return the word vector if it exists, else return a zero vector\n",
        "        return self.word_vectors[word] if word in self.word_vectors else np.zeros(self.word_vectors.vector_size)\n",
        "\n",
        "    def get_avg_vector(self, phrase):\n",
        "        #split phrase into words and obtain their vectors\n",
        "        words = phrase.split()\n",
        "        vectors = [self.get_vector(word) for word in words]\n",
        "        #compute the mean of the vectors if the phrase is not empty\n",
        "        return np.mean(vectors, axis=0) if vectors else np.zeros(self.word_vectors.vector_size)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        #word embeddings does not require learning anything from the training data\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        #initialise an empty list to store word vector\n",
        "        word_embed = []\n",
        "\n",
        "        for _, row in X.iterrows():\n",
        "            head_vector = self.get_avg_vector(row['head'])\n",
        "            tail_vector = self.get_avg_vector(row['tail'])\n",
        "            #concatenate the vectors for head and tail\n",
        "            combined_vector = np.concatenate([head_vector, tail_vector])\n",
        "            word_embed.append(combined_vector)\n",
        "\n",
        "        #convert the list of word embeddings to a DataFrame\n",
        "        feature_names = [f'embedding_{i}' for i in range(len(word_embed[0]))]\n",
        "        return pd.DataFrame(word_embed, columns=feature_names)\n"
      ],
      "metadata": {
        "id": "Bb5hOfYtKNcj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BU8DNlgAb-X"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initiate TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)"
      ],
      "metadata": {
        "id": "toelvqKkLChg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsI8oKCSKCPK"
      },
      "source": [
        "## Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B3ZKFDJpnUZh"
      },
      "outputs": [],
      "source": [
        "#scale using MaxAbsScaler\n",
        "scaler = MaxAbsScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Modelling**"
      ],
      "metadata": {
        "id": "Co7V0cG93V5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "57YjZCAC5mpm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ePyBVSNUmqRe"
      },
      "outputs": [],
      "source": [
        "#drop non-numerical features, except 'text' which will be used for TF-IDF\n",
        "X = fewrel_df.drop(['token', 'relation'], axis=1)\n",
        "y= fewrel_df['relation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yyekW6ufmqRk"
      },
      "outputs": [],
      "source": [
        "#split train and test set, stratifying by y to ensure the label instance is balanced\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a column transformer to assign different transformer to different column\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ner', ner_count_transformer(), ['head', 'tail']),\n",
        "    ('pos', pos_count_transformer(), ['text', 'h_seq', 't_seq']),\n",
        "    ('dependency', dependency_count_transformer(), ['text', 'h_seq', 't_seq']),\n",
        "    ('distance', distance_transformer(), ['h_seq', 't_seq']),\n",
        "    ('word_embedding', word_embedding_transformer(word_vectors=api.load('glove-wiki-gigaword-100')), ['head', 'tail']),\n",
        "    ('tf-idf', tfidf_vectorizer, 'text')\n",
        "], remainder='drop')"
      ],
      "metadata": {
        "id": "8_qLyvzUW8cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2989b97-dd31-42a2-8422-8b27a026d1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create a pipeline to predict RE\n",
        "pipeline = Pipeline([\n",
        "    ('feature_extraction', column_transformer),\n",
        "    ('scale', scaler),\n",
        "    #enable soft classification to get the probability of the prediction\n",
        "    ('svm_model', SVC(kernel='rbf', probability=True, random_state=42))\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "w-djowJFW8Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training model\n",
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "bSWPAk-cn1kL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "5ff135a0-7afb-4b64-e5ec-af91eadcbff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('feature_extraction',\n",
              "                 ColumnTransformer(transformers=[('ner',\n",
              "                                                  ner_count_transformer(),\n",
              "                                                  ['head', 'tail']),\n",
              "                                                 ('pos',\n",
              "                                                  pos_count_transformer(),\n",
              "                                                  ['text', 'h_seq', 't_seq']),\n",
              "                                                 ('dependency',\n",
              "                                                  dependency_count_transformer(),\n",
              "                                                  ['text', 'h_seq', 't_seq']),\n",
              "                                                 ('distance',\n",
              "                                                  distance_transformer(),\n",
              "                                                  ['h_seq', 't_seq']),\n",
              "                                                 ('word_embedding',\n",
              "                                                  word_embedding_transformer(word_vectors=<gensim.models.keyedvectors.KeyedVectors object at 0x7f742c926470>),\n",
              "                                                  ['head', 'tail']),\n",
              "                                                 ('tf-idf',\n",
              "                                                  TfidfVectorizer(max_features=10000,\n",
              "                                                                  stop_words='english'),\n",
              "                                                  'text')])),\n",
              "                ('scale', MaxAbsScaler()),\n",
              "                ('svm_model',\n",
              "                 SVC(C=1, kernel='linear', probability=True, random_state=42))])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;feature_extraction&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;ner&#x27;,\n",
              "                                                  ner_count_transformer(),\n",
              "                                                  [&#x27;head&#x27;, &#x27;tail&#x27;]),\n",
              "                                                 (&#x27;pos&#x27;,\n",
              "                                                  pos_count_transformer(),\n",
              "                                                  [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                                 (&#x27;dependency&#x27;,\n",
              "                                                  dependency_count_transformer(),\n",
              "                                                  [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                                 (&#x27;distance&#x27;,\n",
              "                                                  distance_transformer(),\n",
              "                                                  [&#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                                 (&#x27;word_embedding&#x27;,\n",
              "                                                  word_embedding_transformer(word_vectors=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7f742c926470&gt;),\n",
              "                                                  [&#x27;head&#x27;, &#x27;tail&#x27;]),\n",
              "                                                 (&#x27;tf-idf&#x27;,\n",
              "                                                  TfidfVectorizer(max_features=10000,\n",
              "                                                                  stop_words=&#x27;english&#x27;),\n",
              "                                                  &#x27;text&#x27;)])),\n",
              "                (&#x27;scale&#x27;, MaxAbsScaler()),\n",
              "                (&#x27;svm_model&#x27;,\n",
              "                 SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;feature_extraction&#x27;,\n",
              "                 ColumnTransformer(transformers=[(&#x27;ner&#x27;,\n",
              "                                                  ner_count_transformer(),\n",
              "                                                  [&#x27;head&#x27;, &#x27;tail&#x27;]),\n",
              "                                                 (&#x27;pos&#x27;,\n",
              "                                                  pos_count_transformer(),\n",
              "                                                  [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                                 (&#x27;dependency&#x27;,\n",
              "                                                  dependency_count_transformer(),\n",
              "                                                  [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                                 (&#x27;distance&#x27;,\n",
              "                                                  distance_transformer(),\n",
              "                                                  [&#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                                 (&#x27;word_embedding&#x27;,\n",
              "                                                  word_embedding_transformer(word_vectors=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7f742c926470&gt;),\n",
              "                                                  [&#x27;head&#x27;, &#x27;tail&#x27;]),\n",
              "                                                 (&#x27;tf-idf&#x27;,\n",
              "                                                  TfidfVectorizer(max_features=10000,\n",
              "                                                                  stop_words=&#x27;english&#x27;),\n",
              "                                                  &#x27;text&#x27;)])),\n",
              "                (&#x27;scale&#x27;, MaxAbsScaler()),\n",
              "                (&#x27;svm_model&#x27;,\n",
              "                 SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">feature_extraction: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;ner&#x27;, ner_count_transformer(),\n",
              "                                 [&#x27;head&#x27;, &#x27;tail&#x27;]),\n",
              "                                (&#x27;pos&#x27;, pos_count_transformer(),\n",
              "                                 [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                (&#x27;dependency&#x27;, dependency_count_transformer(),\n",
              "                                 [&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                (&#x27;distance&#x27;, distance_transformer(),\n",
              "                                 [&#x27;h_seq&#x27;, &#x27;t_seq&#x27;]),\n",
              "                                (&#x27;word_embedding&#x27;,\n",
              "                                 word_embedding_transformer(word_vectors=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7f742c926470&gt;),\n",
              "                                 [&#x27;head&#x27;, &#x27;tail&#x27;]),\n",
              "                                (&#x27;tf-idf&#x27;,\n",
              "                                 TfidfVectorizer(max_features=10000,\n",
              "                                                 stop_words=&#x27;english&#x27;),\n",
              "                                 &#x27;text&#x27;)])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ner</label><div class=\"sk-toggleable__content\"><pre>[&#x27;head&#x27;, &#x27;tail&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ner_count_transformer</label><div class=\"sk-toggleable__content\"><pre>ner_count_transformer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pos</label><div class=\"sk-toggleable__content\"><pre>[&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">pos_count_transformer</label><div class=\"sk-toggleable__content\"><pre>pos_count_transformer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">dependency</label><div class=\"sk-toggleable__content\"><pre>[&#x27;text&#x27;, &#x27;h_seq&#x27;, &#x27;t_seq&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">dependency_count_transformer</label><div class=\"sk-toggleable__content\"><pre>dependency_count_transformer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">distance</label><div class=\"sk-toggleable__content\"><pre>[&#x27;h_seq&#x27;, &#x27;t_seq&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">distance_transformer</label><div class=\"sk-toggleable__content\"><pre>distance_transformer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">word_embedding</label><div class=\"sk-toggleable__content\"><pre>[&#x27;head&#x27;, &#x27;tail&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">word_embedding_transformer</label><div class=\"sk-toggleable__content\"><pre>word_embedding_transformer(word_vectors=&lt;gensim.models.keyedvectors.KeyedVectors object at 0x7f742c926470&gt;)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">tf-idf</label><div class=\"sk-toggleable__content\"><pre>text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-30\" type=\"checkbox\" ><label for=\"sk-estimator-id-30\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=10000, stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-31\" type=\"checkbox\" ><label for=\"sk-estimator-id-31\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MaxAbsScaler</label><div class=\"sk-toggleable__content\"><pre>MaxAbsScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" ><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, kernel=&#x27;linear&#x27;, probability=True, random_state=42)</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on Training Data"
      ],
      "metadata": {
        "id": "LRuBb_49N0EB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict on the training set\n",
        "y_pred_train = pipeline.predict(X_train)"
      ],
      "metadata": {
        "id": "qbp83M6SDLR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model\n",
        "print(classification_report(y_train, y_pred_train))"
      ],
      "metadata": {
        "id": "gNw8VWjdDlMS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d586f8a-ae08-4c90-eb2f-98d3c4d03d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       P1001       1.00      1.00      1.00         7\n",
            "        P101       1.00      1.00      1.00         7\n",
            "        P102       1.00      1.00      1.00         7\n",
            "        P105       1.00      1.00      1.00         7\n",
            "        P106       1.00      1.00      1.00         7\n",
            "        P118       1.00      1.00      1.00         7\n",
            "        P123       1.00      1.00      1.00         7\n",
            "        P127       1.00      1.00      1.00         7\n",
            "       P1303       1.00      1.00      1.00         7\n",
            "        P131       1.00      1.00      1.00         7\n",
            "       P1344       1.00      1.00      1.00         7\n",
            "       P1346       1.00      1.00      1.00         7\n",
            "        P135       1.00      1.00      1.00         7\n",
            "        P136       1.00      1.00      1.00         7\n",
            "        P137       1.00      1.00      1.00         7\n",
            "        P140       1.00      1.00      1.00         7\n",
            "       P1408       1.00      1.00      1.00         7\n",
            "       P1411       1.00      1.00      1.00         7\n",
            "       P1435       1.00      1.00      1.00         7\n",
            "        P150       1.00      1.00      1.00         7\n",
            "        P156       1.00      1.00      1.00         7\n",
            "        P159       1.00      1.00      1.00         7\n",
            "         P17       1.00      1.00      1.00         7\n",
            "        P175       1.00      1.00      1.00         7\n",
            "        P176       1.00      1.00      1.00         7\n",
            "        P178       1.00      1.00      1.00         7\n",
            "       P1877       1.00      1.00      1.00         7\n",
            "       P1923       1.00      1.00      1.00         7\n",
            "         P22       1.00      1.00      1.00         7\n",
            "        P241       1.00      1.00      1.00         7\n",
            "        P264       1.00      1.00      1.00         7\n",
            "         P27       1.00      1.00      1.00         7\n",
            "        P276       1.00      1.00      1.00         7\n",
            "        P306       1.00      1.00      1.00         7\n",
            "         P31       1.00      1.00      1.00         7\n",
            "       P3373       1.00      1.00      1.00         7\n",
            "       P3450       1.00      1.00      1.00         7\n",
            "        P355       1.00      1.00      1.00         7\n",
            "         P39       1.00      1.00      1.00         7\n",
            "        P400       1.00      1.00      1.00         7\n",
            "        P403       1.00      1.00      1.00         7\n",
            "        P407       1.00      1.00      1.00         7\n",
            "        P449       1.00      1.00      1.00         7\n",
            "       P4552       1.00      1.00      1.00         7\n",
            "        P460       1.00      1.00      1.00         7\n",
            "        P466       1.00      1.00      1.00         7\n",
            "        P495       1.00      1.00      1.00         7\n",
            "        P527       1.00      1.00      1.00         7\n",
            "        P551       1.00      1.00      1.00         7\n",
            "         P57       1.00      1.00      1.00         7\n",
            "         P58       1.00      1.00      1.00         7\n",
            "          P6       1.00      1.00      1.00         7\n",
            "        P674       1.00      1.00      1.00         7\n",
            "        P706       1.00      1.00      1.00         7\n",
            "        P710       1.00      1.00      1.00         7\n",
            "        P740       1.00      1.00      1.00         7\n",
            "        P750       1.00      1.00      1.00         7\n",
            "        P800       1.00      1.00      1.00         7\n",
            "         P84       1.00      1.00      1.00         7\n",
            "         P86       1.00      1.00      1.00         7\n",
            "        P931       1.00      1.00      1.00         7\n",
            "        P937       1.00      1.00      1.00         7\n",
            "        P974       1.00      1.00      1.00         7\n",
            "        P991       1.00      1.00      1.00         7\n",
            "\n",
            "    accuracy                           1.00       448\n",
            "   macro avg       1.00      1.00      1.00       448\n",
            "weighted avg       1.00      1.00      1.00       448\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction on Held-Out Data"
      ],
      "metadata": {
        "id": "8IKyOAY_N9oJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predict on the test set\n",
        "y_pred_test = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "z1tp_FGIPJlC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate the model\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac96ca9e-94d6-4cd3-b8a6-67743b2c859f",
        "id": "l47sOQUGPJlC"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       P1001       0.77      0.82      0.80       140\n",
            "        P101       0.77      0.76      0.77       140\n",
            "        P102       0.96      0.94      0.95       140\n",
            "        P105       0.99      0.99      0.99       140\n",
            "        P106       0.95      0.90      0.92       140\n",
            "        P118       0.96      0.96      0.96       140\n",
            "        P123       0.63      0.70      0.66       140\n",
            "        P127       0.48      0.48      0.48       140\n",
            "       P1303       0.97      0.99      0.98       140\n",
            "        P131       0.56      0.62      0.59       140\n",
            "       P1344       0.95      0.96      0.96       140\n",
            "       P1346       0.73      0.82      0.77       140\n",
            "        P135       0.83      0.94      0.88       140\n",
            "        P136       0.91      0.85      0.88       140\n",
            "        P137       0.72      0.79      0.75       140\n",
            "        P140       0.92      0.96      0.94       140\n",
            "       P1408       0.89      0.96      0.92       140\n",
            "       P1411       0.99      0.99      0.99       140\n",
            "       P1435       0.99      0.99      0.99       140\n",
            "        P150       0.84      0.84      0.84       140\n",
            "        P156       0.68      0.71      0.69       140\n",
            "        P159       0.63      0.57      0.60       140\n",
            "         P17       0.69      0.70      0.70       140\n",
            "        P175       0.65      0.68      0.66       140\n",
            "        P176       0.82      0.77      0.80       140\n",
            "        P178       0.62      0.59      0.60       140\n",
            "       P1877       0.68      0.70      0.69       140\n",
            "       P1923       0.83      0.85      0.84       140\n",
            "         P22       0.67      0.70      0.68       140\n",
            "        P241       0.93      0.94      0.94       140\n",
            "        P264       0.94      0.87      0.90       140\n",
            "         P27       0.78      0.80      0.79       140\n",
            "        P276       0.65      0.61      0.63       140\n",
            "        P306       0.84      0.93      0.88       140\n",
            "         P31       0.73      0.69      0.71       140\n",
            "       P3373       0.71      0.71      0.71       140\n",
            "       P3450       0.96      0.97      0.97       140\n",
            "        P355       0.74      0.79      0.76       140\n",
            "         P39       0.94      0.94      0.94       140\n",
            "        P400       0.90      0.81      0.85       140\n",
            "        P403       0.78      0.75      0.77       140\n",
            "        P407       0.83      0.88      0.85       140\n",
            "        P449       0.92      0.95      0.93       140\n",
            "       P4552       0.89      0.94      0.91       140\n",
            "        P460       0.60      0.62      0.61       140\n",
            "        P466       0.84      0.81      0.83       140\n",
            "        P495       0.74      0.73      0.74       140\n",
            "        P527       0.54      0.49      0.52       140\n",
            "        P551       0.56      0.61      0.58       140\n",
            "         P57       0.71      0.65      0.68       140\n",
            "         P58       0.57      0.59      0.58       140\n",
            "          P6       0.90      0.95      0.93       140\n",
            "        P674       0.75      0.72      0.74       140\n",
            "        P706       0.80      0.76      0.78       140\n",
            "        P710       0.78      0.72      0.75       140\n",
            "        P740       0.78      0.62      0.69       140\n",
            "        P750       0.86      0.76      0.81       140\n",
            "        P800       0.83      0.77      0.80       140\n",
            "         P84       0.89      0.86      0.88       140\n",
            "         P86       0.80      0.73      0.76       140\n",
            "        P931       0.90      0.91      0.90       140\n",
            "        P937       0.75      0.76      0.76       140\n",
            "        P974       0.75      0.80      0.78       140\n",
            "        P991       0.98      0.98      0.98       140\n",
            "\n",
            "    accuracy                           0.80      8960\n",
            "   macro avg       0.80      0.80      0.80      8960\n",
            "weighted avg       0.80      0.80      0.80      8960\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Hyper-Parameter Tuning**"
      ],
      "metadata": {
        "id": "Hd3DueQKdFV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper-parameter grid\n",
        "#using only combination of few hyper parameters due to computational limitation\n",
        "param_grid = {\n",
        "    'svm_model__C': [0.1, 1],\n",
        "    'svm_model__kernel': ['linear', 'rbf'],\n",
        "}\n",
        "\n",
        "#initialise stratifiedkfold\n",
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "f1_scorer = make_scorer(f1_score, average='weighted')\n",
        "\n",
        "#initialise random search\n",
        "gs = GridSearchCV(pipeline, param_grid, cv=skf, scoring=f1_scorer, verbose=2)"
      ],
      "metadata": {
        "id": "St_5geBQdJnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit random search to train data\n",
        "gs.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "RkvBF-hAdqLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best hyper-parameters\n",
        "print('Best Hyper-Parameters:', gs.best_params_)\n",
        "#best f1 score\n",
        "print('Best F1-Score:', gs.best_score_)\n",
        "#best model\n",
        "best_svm = gs.best_estimator_"
      ],
      "metadata": {
        "id": "n5bCr4iqdqlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "dump(pipeline, 'svm_fewrel_pipeline.pkl')\n",
        "\n",
        "#download\n",
        "from google.colab import files\n",
        "files.download('svm_fewrel_pipeline.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a-YgDwUs69Q9",
        "outputId": "a280bbc4-a4fa-4ee6-fba2-8f27eb8db3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ece3474f-943e-43d3-b609-d2525c146d38\", \"svm_fewrel_pipeline.pkl\", 433077242)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Real-Time Test**"
      ],
      "metadata": {
        "id": "dc4BBjPvNSq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#input new sentence and its head and tail\n",
        "head = ['margaret ekpo international airport']\n",
        "tail = ['calabar']\n",
        "text = ['Nearby margaret airport include Akwa Ibom Airport at Okobo and Margaret Ekpo International Airport in Calabar .']"
      ],
      "metadata": {
        "id": "4hEuLNVf5ScA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clone fewrel repository\n",
        "!git clone https://github.com/thunlp/FewRel\n",
        "#change directory\n",
        "%cd FewRel\n",
        "#load relation explanation\n",
        "with open('./data/pid2name.json', 'r') as file:\n",
        "    relation = json.load(file)"
      ],
      "metadata": {
        "id": "c0bvZalYZCAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b6f1da-bc2d-4201-fe46-d983e69e5990"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FewRel'...\n",
            "remote: Enumerating objects: 565, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 565 (delta 1), reused 0 (delta 0), pack-reused 559\u001b[K\n",
            "Receiving objects: 100% (565/565), 24.68 MiB | 14.92 MiB/s, done.\n",
            "Resolving deltas: 100% (340/340), done.\n",
            "/content/FewRel/FewRel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load pipeline.pkl\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = '/content/drive/My Drive/svm_fewrel_pipeline.pkl'\n",
        "pipeline = load(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC64rg2Iw8OL",
        "outputId": "9c521776-814b-41ae-b76c-e59f146f0cd8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set up environment\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "#define a function to predict relation extraction in real time\n",
        "#make sure the required libraries and relation dataset are imported\n",
        "def prediction_re(head, tail, text):\n",
        "  #convert the input to a dataframe\n",
        "  df = pd.DataFrame({'head':head, 'tail': tail, 'text':text})\n",
        "\n",
        "\n",
        "\n",
        "  #define a function to extract head and tail sequences\n",
        "  def word_sequences(df, text_col='text', head_col='head', tail_col='tail'):\n",
        "\n",
        "      new_h_seqs = []\n",
        "      new_t_seqs = []\n",
        "\n",
        "      for _, row in df.iterrows():\n",
        "          #initialise documents\n",
        "          doc = nlp(row[text_col])\n",
        "          phrase_head = nlp(row[head_col])\n",
        "          phrase_tail = nlp(row[tail_col])\n",
        "\n",
        "          # convert tokens to lowercase strings for comparison\n",
        "          doc_words = [token.text.lower() for token in doc]\n",
        "          head_words = [token.text.lower() for token in phrase_head]\n",
        "          tail_words = [token.text.lower() for token in phrase_tail]\n",
        "\n",
        "          #initialise empty lists to store the sequences\n",
        "          h_seq = []\n",
        "          t_seq = []\n",
        "\n",
        "          #find sequence for the head\n",
        "          for i in range(len(doc_words) - len(head_words) + 1):\n",
        "              if doc_words[i:i + len(head_words)] == head_words:\n",
        "                  h_seq = list(range(i, i + len(head_words)))\n",
        "                  break  # Only find the first occurrence\n",
        "\n",
        "          #find sequence for the tail\n",
        "          for i in range(len(doc_words) - len(tail_words) + 1):\n",
        "              if doc_words[i:i + len(tail_words)] == tail_words:\n",
        "                  t_seq = list(range(i, i + len(tail_words)))\n",
        "                  break  # Only find the first occurrence\n",
        "\n",
        "          #append found sequences\n",
        "          new_h_seqs.append(h_seq)\n",
        "          new_t_seqs.append(t_seq)\n",
        "\n",
        "      #add new sequences to the DataFrame\n",
        "      df['h_seq'] = new_h_seqs\n",
        "      df['t_seq'] = new_t_seqs\n",
        "\n",
        "      return df\n",
        "\n",
        "  #running function to extract head and tail sequences\n",
        "  word_sequences(df)\n",
        "\n",
        "  #load stored pipeline\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  model_path = '/content/drive/My Drive/svm_fewrel_pipeline.pkl'\n",
        "  pipeline = load(model_path)\n",
        "\n",
        "  #use predict_proba to get the probability of the prediction\n",
        "  y_pred_proba = pipeline.predict_proba(df)\n",
        "  max_probabilities = np.max(y_pred_proba, axis=1)\n",
        "  max_classes = np.argmax(y_pred_proba, axis=1)\n",
        "  rel_label = pipeline.classes_\n",
        "\n",
        "  # print('The relation of', \"'\"+text[0]+\"'\", 'is', relation[y_pred[0]][0] +\", \"+ relation[y_pred[0]][1])\n",
        "\n",
        "  for i in range(len(y_pred_proba)):\n",
        "    print(f\"The relation of '{head[i]}' and '{tail[i]}' in the sentence \\n'{text[i]}' is '{rel_label[max_classes][i]}' \\n -Relation Name: {relation[rel_label[max_classes][i]][0]} \\n -Description: {relation[rel_label[max_classes][i]][1]} \\nwith probability {max_probabilities[i]:.2f}\")"
      ],
      "metadata": {
        "id": "HYH-yJFj_1S4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking to run  prediction\n",
        "prediction_re(head, tail, text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG4GH_DKSZWd",
        "outputId": "4f2226a3-3e79-41ae-8ce6-dd0bfe301ece"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "The relation of 'margaret ekpo international airport' and 'calabar' in the sentence \n",
            "'Nearby margaret airport include Akwa Ibom Airport at Okobo and Margaret Ekpo International Airport in Calabar .' is 'P931' \n",
            " -Relation Name: place served by transport hub \n",
            " -Description: territorial entity or entities served by this transport hub (airport, train station, etc.) \n",
            "with probability 1.00\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "temefE88BqPS",
        "taNJ97wVBvAJ",
        "3fwG8jxaDzBn",
        "kYJnAYNmCDXt",
        "ALX-L-MlZ0nX",
        "t3B4A8DHjoUC",
        "Ebgm3m8nr9-q",
        "dZ1o8zms9FpR",
        "1BU8DNlgAb-X",
        "VsI8oKCSKCPK",
        "LRuBb_49N0EB",
        "8IKyOAY_N9oJ",
        "Hd3DueQKdFV4",
        "dc4BBjPvNSq7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}